{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam # I believe this is better optimizer for our case\n",
    "from keras.preprocessing.image import ImageDataGenerator # to augmenting our images for increasing accuracy\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split # to split our train data into train and validation sets\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "np.random.seed(13) # My lucky number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "030b1ebf761aa450ce18b253813433ca5293c2fc"
   },
   "outputs": [],
   "source": [
    "num_classes = 10 # We have 10 digits to identify\n",
    "batch_size = 128 # Handle 128 pictures at each round\n",
    "epochs = 50 # 50 Epoch is enough\n",
    "img_rows, img_cols = 28, 28 # Image dimensions 28 pixels in height&width\n",
    "input_shape = (img_rows, img_cols,1) # We'll use this while building layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "dc20a3a5e6d491e98be7d0ff3aaa6eec7bdc123b"
   },
   "outputs": [],
   "source": [
    "# Load some date to rock'n roll\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "23e65adaf0ef381576c5597aebb6ce5dcc466174"
   },
   "outputs": [],
   "source": [
    "# Drop the label from the data and move it to real label part\n",
    "y_train = train[\"label\"]\n",
    "x_train = train.drop(labels = [\"label\"],axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "fc3b21df164c213db201b317ba453c8914dd2200"
   },
   "outputs": [],
   "source": [
    "# Normalize both sets\n",
    "x_train /= 255\n",
    "test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "922cb2c73388a6ec4b368b032f133b2c4c9783c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 train samples\n",
      "28000 test samples\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape[0], 'train samples')\n",
    "print(test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "4ac625840bb33a39580405985955d9076c003d3c"
   },
   "outputs": [],
   "source": [
    "# Images should be in shape of height,width and color channel so it will be 28x28x1\n",
    "x_train = x_train.values.reshape(-1,img_rows,img_cols,1).astype('float32')\n",
    "test = test.values.reshape(-1,img_rows,img_cols,1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "057065fd2bf1e11bdb9885b746c27452ee21505c"
   },
   "outputs": [],
   "source": [
    "# Class vectors needs to be categorical so we use \"to_categorical\" function of keras utilities for categorical encoding\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets split our train set into train and validation test sets with my lucky number 13 :)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "1e271e0d37535c7c401a873578ad3dd19b51b133"
   },
   "outputs": [],
   "source": [
    "def model_cnn(input_shape=input_shape, num_classes=num_classes):   \n",
    "    model = Sequential()\n",
    "\n",
    "    # Add convolutional layer consisting of 32 filters and shape of 3x3 with ReLU activation\n",
    "    # We want to preserve more information for following layers so we use padding\n",
    "    # 'Same' padding tries to pad evenly left and right, \n",
    "    # but if the amount of columns to be added is odd, it will add the extra column to the right\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation='relu', input_shape = input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Add convolutional layer consisting of 32 filters and shape of 5x5 with ReLU activation\n",
    "    # We give strides=2 for space between each sample on the pixel grid\n",
    "    model.add(Conv2D(32, kernel_size = (5,5), strides=2, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Dropping %40 of neurons\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = (5,5), strides=2, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    # To be able to merge into fully connected layer we have to flatten\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    # Lets add softmax activated neurons as much as number of classes\n",
    "    model.add(Dense(num_classes, activation = \"softmax\"))\n",
    "    # Compile the model with loss and metrics\n",
    "    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5(input_shape=input_shape,num_classes=num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape, padding=\"same\"))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "    model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "    model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Custom CNN Network:\n"
     ]
    }
   ],
   "source": [
    "print(\"My Custom CNN Network:\")\n",
    "plot_model(model_cnn(),to_file='custom-cnn.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"custom-cnn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Yann LeCun's LeNet-5 Network:\n"
     ]
    }
   ],
   "source": [
    "print(\"Master Yann LeCun's LeNet-5 Network:\")\n",
    "plot_model(LeNet5(), to_file='lenet-5.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lenet-5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = []\n",
    "model.append(model_cnn())\n",
    "model.append(LeNet5())\n",
    "# You can add more models like inception, vgg-16/vgg-19 etc. to improve overall accuracy during ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3b5468b524407441d9731b87e97bb010b7cded69"
   },
   "outputs": [],
   "source": [
    "# Generate batches of tensor image data with real-time data augmentation more detail: https://keras.io/preprocessing/image/\n",
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range = 0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 400s - loss: 0.5816 - acc: 0.8225 - val_loss: 0.0881 - val_acc: 0.9717\n",
      "Epoch 2/50\n",
      " - 360s - loss: 0.1396 - acc: 0.9580 - val_loss: 0.0639 - val_acc: 0.9788\n",
      "Epoch 3/50\n",
      " - 384s - loss: 0.1072 - acc: 0.9675 - val_loss: 0.0597 - val_acc: 0.9817\n",
      "Epoch 4/50\n",
      " - 326s - loss: 0.0851 - acc: 0.9750 - val_loss: 0.0352 - val_acc: 0.9890\n",
      "Epoch 5/50\n",
      " - 307s - loss: 0.0751 - acc: 0.9776 - val_loss: 0.0407 - val_acc: 0.9874\n",
      "Epoch 6/50\n",
      " - 292s - loss: 0.0712 - acc: 0.9787 - val_loss: 0.0304 - val_acc: 0.9907\n",
      "Epoch 7/50\n",
      " - 300s - loss: 0.0606 - acc: 0.9819 - val_loss: 0.0223 - val_acc: 0.9929\n",
      "Epoch 8/50\n",
      " - 308s - loss: 0.0608 - acc: 0.9813 - val_loss: 0.0181 - val_acc: 0.9950\n",
      "Epoch 9/50\n",
      " - 288s - loss: 0.0551 - acc: 0.9837 - val_loss: 0.0320 - val_acc: 0.9914\n",
      "Epoch 10/50\n",
      " - 315s - loss: 0.0515 - acc: 0.9847 - val_loss: 0.0246 - val_acc: 0.9914\n",
      "Epoch 11/50\n",
      " - 526s - loss: 0.0485 - acc: 0.9855 - val_loss: 0.0457 - val_acc: 0.9860\n",
      "Epoch 12/50\n",
      " - 295s - loss: 0.0492 - acc: 0.9852 - val_loss: 0.0234 - val_acc: 0.9931\n",
      "Epoch 13/50\n",
      " - 290s - loss: 0.0453 - acc: 0.9865 - val_loss: 0.0266 - val_acc: 0.9919\n",
      "Epoch 14/50\n",
      " - 287s - loss: 0.0418 - acc: 0.9873 - val_loss: 0.0197 - val_acc: 0.9948\n",
      "Epoch 15/50\n",
      " - 293s - loss: 0.0419 - acc: 0.9875 - val_loss: 0.0177 - val_acc: 0.9943\n",
      "Epoch 16/50\n",
      " - 293s - loss: 0.0373 - acc: 0.9884 - val_loss: 0.0186 - val_acc: 0.9945\n",
      "Epoch 17/50\n",
      " - 265s - loss: 0.0376 - acc: 0.9892 - val_loss: 0.0204 - val_acc: 0.9936\n",
      "Epoch 18/50\n",
      " - 291s - loss: 0.0379 - acc: 0.9890 - val_loss: 0.0212 - val_acc: 0.9938\n",
      "Epoch 19/50\n",
      " - 298s - loss: 0.0373 - acc: 0.9886 - val_loss: 0.0201 - val_acc: 0.9945\n",
      "Epoch 20/50\n",
      " - 306s - loss: 0.0271 - acc: 0.9921 - val_loss: 0.0138 - val_acc: 0.9971\n",
      "Epoch 21/50\n",
      " - 343s - loss: 0.0232 - acc: 0.9934 - val_loss: 0.0123 - val_acc: 0.9974\n",
      "Epoch 22/50\n",
      " - 287s - loss: 0.0218 - acc: 0.9931 - val_loss: 0.0129 - val_acc: 0.9974\n",
      "Epoch 23/50\n",
      " - 264s - loss: 0.0202 - acc: 0.9941 - val_loss: 0.0129 - val_acc: 0.9969\n",
      "Epoch 24/50\n",
      " - 296s - loss: 0.0225 - acc: 0.9932 - val_loss: 0.0125 - val_acc: 0.9967\n",
      "Epoch 25/50\n",
      " - 288s - loss: 0.0197 - acc: 0.9943 - val_loss: 0.0116 - val_acc: 0.9971\n",
      "Epoch 26/50\n",
      " - 299s - loss: 0.0183 - acc: 0.9944 - val_loss: 0.0135 - val_acc: 0.9969\n",
      "Epoch 27/50\n",
      " - 292s - loss: 0.0186 - acc: 0.9948 - val_loss: 0.0127 - val_acc: 0.9969\n",
      "Epoch 28/50\n",
      " - 287s - loss: 0.0191 - acc: 0.9942 - val_loss: 0.0128 - val_acc: 0.9974\n",
      "Epoch 29/50\n",
      " - 289s - loss: 0.0177 - acc: 0.9937 - val_loss: 0.0131 - val_acc: 0.9962\n",
      "Epoch 30/50\n",
      " - 297s - loss: 0.0204 - acc: 0.9943 - val_loss: 0.0124 - val_acc: 0.9971\n",
      "Epoch 31/50\n",
      " - 301s - loss: 0.0177 - acc: 0.9946 - val_loss: 0.0126 - val_acc: 0.9967\n",
      "Epoch 32/50\n",
      " - 330s - loss: 0.0189 - acc: 0.9943 - val_loss: 0.0124 - val_acc: 0.9971\n",
      "Epoch 33/50\n",
      " - 293s - loss: 0.0160 - acc: 0.9948 - val_loss: 0.0121 - val_acc: 0.9967\n",
      "Epoch 34/50\n",
      " - 294s - loss: 0.0161 - acc: 0.9948 - val_loss: 0.0122 - val_acc: 0.9969\n",
      "Epoch 35/50\n",
      " - 299s - loss: 0.0156 - acc: 0.9954 - val_loss: 0.0121 - val_acc: 0.9969\n",
      "Epoch 36/50\n",
      " - 319s - loss: 0.0160 - acc: 0.9952 - val_loss: 0.0122 - val_acc: 0.9969\n",
      "Epoch 37/50\n",
      " - 327s - loss: 0.0165 - acc: 0.9953 - val_loss: 0.0121 - val_acc: 0.9974\n",
      "Epoch 38/50\n",
      " - 402s - loss: 0.0174 - acc: 0.9950 - val_loss: 0.0120 - val_acc: 0.9974\n",
      "Epoch 39/50\n",
      " - 465s - loss: 0.0170 - acc: 0.9950 - val_loss: 0.0120 - val_acc: 0.9974\n",
      "Epoch 40/50\n",
      " - 284s - loss: 0.0160 - acc: 0.9952 - val_loss: 0.0120 - val_acc: 0.9974\n",
      "Epoch 41/50\n",
      " - 287s - loss: 0.0152 - acc: 0.9952 - val_loss: 0.0119 - val_acc: 0.9974\n",
      "Epoch 42/50\n",
      " - 283s - loss: 0.0173 - acc: 0.9948 - val_loss: 0.0120 - val_acc: 0.9974\n",
      "Epoch 43/50\n",
      " - 298s - loss: 0.0157 - acc: 0.9953 - val_loss: 0.0119 - val_acc: 0.9974\n",
      "Epoch 44/50\n",
      " - 286s - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0118 - val_acc: 0.9974\n",
      "Epoch 45/50\n",
      " - 299s - loss: 0.0151 - acc: 0.9958 - val_loss: 0.0118 - val_acc: 0.9974\n",
      "Epoch 46/50\n",
      " - 294s - loss: 0.0166 - acc: 0.9951 - val_loss: 0.0119 - val_acc: 0.9974\n",
      "Epoch 47/50\n",
      " - 293s - loss: 0.0156 - acc: 0.9954 - val_loss: 0.0118 - val_acc: 0.9974\n",
      "Epoch 48/50\n",
      " - 294s - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0118 - val_acc: 0.9974\n",
      "Epoch 49/50\n",
      " - 298s - loss: 0.0161 - acc: 0.9949 - val_loss: 0.0119 - val_acc: 0.9974\n",
      "Epoch 50/50\n",
      " - 308s - loss: 0.0154 - acc: 0.9956 - val_loss: 0.0120 - val_acc: 0.9974\n",
      "Epoch 1/50\n",
      " - 72s - loss: 0.4572 - acc: 0.8550 - val_loss: 0.0877 - val_acc: 0.9755\n",
      "Epoch 2/50\n",
      " - 74s - loss: 0.1436 - acc: 0.9563 - val_loss: 0.0693 - val_acc: 0.9769\n",
      "Epoch 3/50\n",
      " - 71s - loss: 0.1039 - acc: 0.9690 - val_loss: 0.0493 - val_acc: 0.9845\n",
      "Epoch 4/50\n",
      " - 73s - loss: 0.0867 - acc: 0.9739 - val_loss: 0.0650 - val_acc: 0.9800\n",
      "Epoch 5/50\n",
      " - 69s - loss: 0.0717 - acc: 0.9782 - val_loss: 0.0443 - val_acc: 0.9855\n",
      "Epoch 6/50\n",
      " - 69s - loss: 0.0623 - acc: 0.9804 - val_loss: 0.0469 - val_acc: 0.9857\n",
      "Epoch 7/50\n",
      " - 72s - loss: 0.0561 - acc: 0.9829 - val_loss: 0.0437 - val_acc: 0.9876\n",
      "Epoch 8/50\n",
      " - 72s - loss: 0.0550 - acc: 0.9839 - val_loss: 0.0424 - val_acc: 0.9876\n",
      "Epoch 9/50\n",
      " - 70s - loss: 0.0481 - acc: 0.9850 - val_loss: 0.0332 - val_acc: 0.9898\n",
      "Epoch 10/50\n",
      " - 69s - loss: 0.0459 - acc: 0.9858 - val_loss: 0.0315 - val_acc: 0.9898\n",
      "Epoch 11/50\n",
      " - 75s - loss: 0.0414 - acc: 0.9880 - val_loss: 0.0308 - val_acc: 0.9893\n",
      "Epoch 12/50\n",
      " - 72s - loss: 0.0388 - acc: 0.9879 - val_loss: 0.0490 - val_acc: 0.9855\n",
      "Epoch 13/50\n",
      " - 73s - loss: 0.0391 - acc: 0.9880 - val_loss: 0.0272 - val_acc: 0.9919\n",
      "Epoch 14/50\n",
      " - 72s - loss: 0.0369 - acc: 0.9886 - val_loss: 0.0271 - val_acc: 0.9919\n",
      "Epoch 15/50\n",
      " - 74s - loss: 0.0359 - acc: 0.9892 - val_loss: 0.0242 - val_acc: 0.9914\n",
      "Epoch 16/50\n",
      " - 73s - loss: 0.0337 - acc: 0.9895 - val_loss: 0.0301 - val_acc: 0.9917\n",
      "Epoch 17/50\n",
      " - 72s - loss: 0.0314 - acc: 0.9903 - val_loss: 0.0350 - val_acc: 0.9886\n",
      "Epoch 18/50\n",
      " - 74s - loss: 0.0295 - acc: 0.9907 - val_loss: 0.0241 - val_acc: 0.9921\n",
      "Epoch 19/50\n",
      " - 72s - loss: 0.0272 - acc: 0.9916 - val_loss: 0.0309 - val_acc: 0.9893\n",
      "Epoch 20/50\n",
      " - 77s - loss: 0.0282 - acc: 0.9918 - val_loss: 0.0330 - val_acc: 0.9912\n",
      "Epoch 21/50\n",
      " - 73s - loss: 0.0288 - acc: 0.9910 - val_loss: 0.0226 - val_acc: 0.9945\n",
      "Epoch 22/50\n",
      " - 78s - loss: 0.0272 - acc: 0.9914 - val_loss: 0.0244 - val_acc: 0.9926\n",
      "Epoch 23/50\n",
      " - 70s - loss: 0.0169 - acc: 0.9948 - val_loss: 0.0200 - val_acc: 0.9938\n",
      "Epoch 24/50\n",
      " - 76s - loss: 0.0140 - acc: 0.9957 - val_loss: 0.0172 - val_acc: 0.9945\n",
      "Epoch 25/50\n",
      " - 72s - loss: 0.0128 - acc: 0.9963 - val_loss: 0.0174 - val_acc: 0.9943\n",
      "Epoch 26/50\n",
      " - 71s - loss: 0.0125 - acc: 0.9963 - val_loss: 0.0187 - val_acc: 0.9931\n",
      "Epoch 27/50\n",
      " - 74s - loss: 0.0136 - acc: 0.9958 - val_loss: 0.0185 - val_acc: 0.9945\n",
      "Epoch 28/50\n",
      " - 79s - loss: 0.0131 - acc: 0.9961 - val_loss: 0.0176 - val_acc: 0.9933\n",
      "Epoch 29/50\n",
      " - 76s - loss: 0.0121 - acc: 0.9960 - val_loss: 0.0167 - val_acc: 0.9943\n",
      "Epoch 30/50\n",
      " - 80s - loss: 0.0123 - acc: 0.9962 - val_loss: 0.0172 - val_acc: 0.9948\n",
      "Epoch 31/50\n",
      " - 72s - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0166 - val_acc: 0.9943\n",
      "Epoch 32/50\n",
      " - 75s - loss: 0.0108 - acc: 0.9966 - val_loss: 0.0182 - val_acc: 0.9940\n",
      "Epoch 33/50\n",
      " - 73s - loss: 0.0105 - acc: 0.9965 - val_loss: 0.0181 - val_acc: 0.9943\n",
      "Epoch 34/50\n",
      " - 73s - loss: 0.0122 - acc: 0.9963 - val_loss: 0.0181 - val_acc: 0.9948\n",
      "Epoch 35/50\n",
      " - 71s - loss: 0.0112 - acc: 0.9968 - val_loss: 0.0180 - val_acc: 0.9952\n",
      "Epoch 36/50\n",
      " - 76s - loss: 0.0091 - acc: 0.9972 - val_loss: 0.0182 - val_acc: 0.9950\n",
      "Epoch 37/50\n",
      " - 71s - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0183 - val_acc: 0.9950\n",
      "Epoch 38/50\n",
      " - 75s - loss: 0.0096 - acc: 0.9971 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 39/50\n",
      " - 73s - loss: 0.0092 - acc: 0.9972 - val_loss: 0.0185 - val_acc: 0.9950\n",
      "Epoch 40/50\n",
      " - 70s - loss: 0.0092 - acc: 0.9975 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 41/50\n",
      " - 71s - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 42/50\n",
      " - 77s - loss: 0.0091 - acc: 0.9974 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 43/50\n",
      " - 73s - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 44/50\n",
      " - 74s - loss: 0.0092 - acc: 0.9974 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 45/50\n",
      " - 75s - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 46/50\n",
      " - 79s - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 73s - loss: 0.0087 - acc: 0.9974 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 48/50\n",
      " - 72s - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 49/50\n",
      " - 82s - loss: 0.0091 - acc: 0.9971 - val_loss: 0.0184 - val_acc: 0.9950\n",
      "Epoch 50/50\n",
      " - 74s - loss: 0.0107 - acc: 0.9966 - val_loss: 0.0184 - val_acc: 0.9950\n"
     ]
    }
   ],
   "source": [
    "# Start multiple model training with the batch size\n",
    "# Use Reduce LR on Plateau for reducing Learning Rate if there is no decrease at loss for 3 epochs\n",
    "models = []\n",
    "for i in range(len(model)):\n",
    "    model[i].fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n",
    "                                        epochs = epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                                        validation_data = (x_test,y_test), \n",
    "                                        callbacks=[ReduceLROnPlateau(monitor='loss', patience=3, factor=0.1)], \n",
    "                                        verbose=2)\n",
    "    models.append(model[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels with models\n",
    "labels = []\n",
    "for m in models:\n",
    "    predicts = np.argmax(m.predict(test), axis=1)\n",
    "    labels.append(predicts)\n",
    "    \n",
    "# Ensemble with voting\n",
    "labels = np.array(labels)\n",
    "labels = np.transpose(labels, (1, 0))\n",
    "labels = scipy.stats.mode(labels, axis=-1)[0]\n",
    "labels = np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump predictions into submission file for kaggle competition\n",
    "pd.DataFrame({'ImageId' : np.arange(1, predicts.shape[0] + 1), 'Label' : labels }).to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
